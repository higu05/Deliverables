# Install OpenAI library if not installed
!pip install openai

# File: config.py
class Config:
    """Configuration settings for the OpenAI API."""
    OPENAI_API_KEY = "your_openai_api_key"  # Replace with your actual OpenAI API key
    MODEL_NAME = "text-davinci-003"  # Change to other OpenAI models like 'gpt-4' if needed
    TEMPERATURE = 0.7  # Creativity level of the response
    MAX_TOKENS = 100  # Maximum tokens for the output
    TOP_P = 1  # Sampling strategy
    FREQUENCY_PENALTY = 0  # Penalty for repeating words
    PRESENCE_PENALTY = 0  # Penalty for repeating ideas

# File: model.py
import openai

class LLMModel:
    """Manages interaction with the OpenAI API."""
    def __init__(self, config):
        self.api_key = config.OPENAI_API_KEY
        self.model_name = config.MODEL_NAME
        self.temperature = config.TEMPERATURE
        self.max_tokens = config.MAX_TOKENS
        self.top_p = config.TOP_P
        self.frequency_penalty = config.FREQUENCY_PENALTY
        self.presence_penalty = config.PRESENCE_PENALTY
        openai.api_key = self.api_key

    def generate_response(self, prompt):
        """Generates a response from the OpenAI API based on the input prompt."""
        try:
            response = openai.Completion.create(
                engine=self.model_name,
                prompt=prompt,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                top_p=self.top_p,
                frequency_penalty=self.frequency_penalty,
                presence_penalty=self.presence_penalty,
            )
            return response.choices[0].text.strip()
        except Exception as e:
            return f"Error: {str(e)}"

# File: data_service.py
class DataService:
    """Handles prompt preparation and management."""
    def __init__(self):
        self.prompts = []

    def add_prompt(self, prompt):
        """Adds a new prompt to the list."""
        self.prompts.append(prompt)

    def get_prompts(self):
        """Returns the list of prompts."""
        return self.prompts

# File: controller.py
class ExperimentController:
    """Coordinates the flow of the experiment."""
    def __init__(self, llm_model, data_service):
        self.llm_model = llm_model
        self.data_service = data_service

    def run_experiment(self):
        """Runs the experiment by generating responses for all prompts."""
        prompts = self.data_service.get_prompts()
        if not prompts:
            return "No prompts available to process."
        
        results = []
        for idx, prompt in enumerate(prompts, start=1):
            print(f"Processing Prompt {idx}: {prompt}")
            response = self.llm_model.generate_response(prompt)
            results.append({"Prompt": prompt, "Response": response})
        return results

# Main: project_2_openai_experiment.ipynb
def main():
    # Instantiate components
    config = Config()
    llm_model = LLMModel(config)
    data_service = DataService()
    controller = ExperimentController(llm_model, data_service)

    # Add example prompts
    data_service.add_prompt("What is the capital of France?")
    data_service.add_prompt("Explain the theory of relativity in simple terms.")
    data_service.add_prompt("Write a short story about a talking dog.")
    
    # Run the experiment
    results = controller.run_experiment()

    # Display results
    for result in results:
        print("\n---")
        print(f"Prompt: {result['Prompt']}")
        print(f"Response: {result['Response']}")
        print("---")

# Execute the program
if __name__ == "__main__":
    main()
