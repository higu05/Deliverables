```python
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evaluation Framework\n",
    "Framework for evaluating and comparing different LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install openai anthropic gradio plotly pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-key'\n",
    "os.environ['ANTHROPIC_API_KEY'] = 'your-anthropic-key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    OPENAI_API_KEY: str = os.getenv('OPENAI_API_KEY', '')\n",
    "    ANTHROPIC_API_KEY: str = os.getenv('ANTHROPIC_API_KEY', '')\n",
    "    MODELS: Dict[str, Dict] = {\n",
    "        'gpt-3.5-turbo': {'provider': 'openai', 'max_tokens': 4096},\n",
    "        'gpt-4': {'provider': 'openai', 'max_tokens': 8192},\n",
    "        'claude-3-opus-20240229': {'provider': 'anthropic', 'max_tokens': 4096}\n",
    "    }\n",
    "    METRICS = ['accuracy', 'latency', 'token_usage', 'cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class Prompt:\n",
    "    text: str\n",
    "    expected_response: str = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "@dataclass\n",
    "class ModelResponse:\n",
    "    text: str\n",
    "    model_name: str\n",
    "    latency: float\n",
    "    tokens_used: int\n",
    "    cost: float\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    model_name: str\n",
    "    accuracy: float\n",
    "    avg_latency: float\n",
    "    total_tokens: int\n",
    "    total_cost: float\n",
    "    responses: List[ModelResponse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import openai\n",
    "import anthropic\n",
    "import time\n",
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "class ModelService:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.openai_client = openai.OpenAI(api_key=config.OPENAI_API_KEY)\n",
    "        self.anthropic_client = anthropic.Anthropic(api_key=config.ANTHROPIC_API_KEY)\n",
    "\n",
    "    async def get_response(self, prompt: Prompt, model_name: str) -> ModelResponse:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.config.MODELS[model_name]['provider'] == 'openai':\n",
    "            response = await self._get_openai_response(prompt, model_name)\n",
    "        else:\n",
    "            response = await self._get_anthropic_response(prompt, model_name)\n",
    "            \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        return ModelResponse(\n",
    "            text=response['text'],\n",
    "            model_name=model_name,\n",
    "            latency=latency,\n",
    "            tokens_used=response['tokens'],\n",
    "            cost=self._calculate_cost(model_name, response['tokens'])\n",
    "        )\n",
    "\n",
    "    async def _get_openai_response(self, prompt: Prompt, model_name: str):\n",
    "        response = await self.openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt.text}]\n",
    "        )\n",
    "        return {\n",
    "            'text': response.choices[0].message.content,\n",
    "            'tokens': response.usage.total_tokens\n",
    "        }\n",
    "\n",
    "    async def _get_anthropic_response(self, prompt: Prompt, model_name: str):\n",
    "        response = await self.anthropic_client.messages.create(\n",
    "            model=model_name,\n",
    "            max_tokens=self.config.MODELS[model_name]['max_tokens'],\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt.text}]\n",
    "        )\n",
    "        return {\n",
    "            'text': response.content[0].text,\n",
    "            'tokens': response.usage.output_tokens + response.usage.input_tokens\n",
    "        }\n",
    "\n",
    "    def _calculate_cost(self, model_name: str, tokens: int) -> float:\n",
    "        cost_per_token = {\n",
    "            'gpt-3.5-turbo': 0.000002,\n",
    "            'gpt-4': 0.00003,\n",
    "            'claude-3-opus-20240229': 0.00001\n",
    "        }\n",
    "        return tokens * cost_per_token.get(model_name, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "class EvaluationService:\n",
    "    def __init__(self, config: Config, model_service: ModelService):\n",
    "        self.config = config\n",
    "        self.model_service = model_service\n",
    "\n",
    "    async def evaluate_models(self, prompts: List[Prompt], models: List[str] = None) -> List[EvaluationResult]:\n",
    "        if not models:\n",
    "            models = list(self.config.MODELS.keys())\n",
    "\n",
    "        results = []\n",
    "        for model_name in models:\n",
    "            model_responses = []\n",
    "            for prompt in prompts:\n",
    "                response = await self.model_service.get_response(prompt, model_name)\n",
    "                model_responses.append(response)\n",
    "\n",
    "            results.append(self._calculate_metrics(model_responses, model_name))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _calculate_metrics(self, responses: List[ModelResponse], model_name: str) -> EvaluationResult:\n",
    "        return EvaluationResult(\n",
    "            model_name=model_name,\n",
    "            accuracy=self._calculate_accuracy(responses),\n",
    "            avg_latency=np.mean([r.latency for r in responses]),\n",
    "            total_tokens=sum([r.tokens_used for r in responses]),\n",
    "            total_cost=sum([r.cost for r in responses]),\n",
    "            responses=responses\n",
    "        )\n",
    "\n",
    "    def _calculate_accuracy(self, responses: List[ModelResponse]) -> float:\n",
    "        return np.random.uniform(0.7, 1.0)  # Placeholder for actual metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "class ResultStorage:\n",
    "    def __init__(self, filename: str = 'evaluation_results.json'):\n",
    "        self.filename = filename\n",
    "\n",
    "    def save_results(self, results: List[EvaluationResult]):\n",
    "        with open(self.filename, 'w') as f:\n",
    "            json.dump([self._serialize_result(r) for r in results], f)\n",
    "\n",
    "    def load_results(self) -> List[EvaluationResult]:\n",
    "        try:\n",
    "            with open(self.filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                return [self._deserialize_result(r) for r in data]\n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "\n",
    "    def _serialize_result(self, result: EvaluationResult) -> dict:\n",
    "        return {\n",
    "            'model_name': result.model_name,\n",
    "            'accuracy': result.accuracy,\n",
    "            'avg_latency': result.avg_latency,\n",
    "            'total_tokens': result.total_tokens,\n",
    "            'total_cost': result.total_cost,\n",
    "            'responses': [vars(r) for r in result.responses]\n",
    "        }\n",
    "\n",
    "    def _deserialize_result(self, data: dict) -> EvaluationResult:\n",
    "        return EvaluationResult(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "class EvaluationUI:\n",
    "    def __init__(self, evaluation_service: EvaluationService):\n",
    "        self.evaluation_service = evaluation_service\n",
    "\n",
    "    async def evaluate(self, prompts: str, models: List[str]) -> dict:\n",
    "        prompt_list = [Prompt(text=p.strip()) for p in prompts.split('\\n') if p.strip()]\n",
    "        results = await self.evaluation_service.evaluate_models(prompt_list, models)\n",
    "        return self._create_visualizations(results)\n",
    "\n",
    "    def _create_visualizations(self, results: List[EvaluationResult]) -> dict:\n",
    "        df = pd.DataFrame([{\n",
    "            'Model': r.model_name,\n",
    "            'Accuracy': r.accuracy,\n",
    "            'Latency': r.avg_latency,\n",
    "            'Total Tokens': r.total_tokens,\n",
    "            'Total Cost': r.total_cost\n",
    "        } for r in results])\n",
    "\n",
    "        metrics_plot = px.bar(df, x='Model', y=['Accuracy', 'Latency'], \n",
    "                            title='Model Performance Metrics',\n",
    "                            barmode='group')\n",
    "        \n",
    "        cost_plot = px.bar(df, x='Model', y='Total Cost',\n",
    "                          title='Cost Comparison')\n",
    "\n",
    "        return {\n",
    "            'metrics': metrics_plot,\n",
    "            'cost': cost_plot,\n",
    "            'data': df.to_html()\n",
    "        }\n",
    "\n",
    "    def launch(self):\n",
    "        with gr.Blocks() as interface:\n",
    "            gr.Markdown(\"# LLM Evaluation Framework\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                prompts = gr.Textbox(label=\"Enter prompts (one per line)\", lines=5)\n",
    "                models = gr.CheckboxGroup(\n",
    "                    choices=['gpt-3.5-turbo', 'gpt-4', 'claude-3-opus-20240229'],\n",
    "                    label=\"Select models to evaluate\"\n",
    "                )\n",
    "            \n",
    "            evaluate_btn = gr.Button(\"Run Evaluation\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                metrics_plot = gr.Plot(label=\"Performance Metrics\")\n",
    "                cost_plot = gr.Plot(label=\"Cost Comparison\")\n",
    "            \n",
    "            results_table = gr.HTML(label=\"Detailed Results\")\n",
    "\n",
    "            evaluate_btn.click(\n",
    "                fn=self.evaluate,\n",
    "                inputs=[prompts, models],\n",
    "                outputs=[metrics_plot, cost_plot, results_table]\n",
    "            )\n",
    "\n",
    "        interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def main():\n",
    "    config = Config()\n",
    "    model_service = ModelService(config)\n",
    "    evaluation_service = EvaluationService(config, model_service)\n",
    "    ui = EvaluationUI(evaluation_service)\n",
    "    ui.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  }
 },
 "
```